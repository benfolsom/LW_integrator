"""
Benchmark Script: New Package vs Original Code

This script validates that our new LW integrator package produces
results consistent with the original research code to reasonable precision.

We'll compare:
1. Single integration step results
2. Multi-step trajectory evolution  
3. Energy conservation over time
4. Performance metrics

Author: Ben Folsom (human oversight)
Date: 2025-09-12
"""

import numpy as np
import matplotlib.pyplot as plt
import time
import sys
import os
from typing import Dict, List, Tuple, Any

# Add paths for both new package and original code
sys.path.insert(0, os.path.abspath('.'))
sys.path.insert(0, os.path.abspath('./lw_integrator'))

# Import new package
from lw_integrator.core.integration import Li√©nardWiechertIntegrator
from lw_integrator.core.optimized_integration import OptimizedLi√©nardWiechertIntegrator
from lw_integrator.physics.constants import *

# Import original code modules
try:
    import covariant_integrator_library as original_lib
    import bunch_inits
    ORIGINAL_AVAILABLE = True
    print("‚úÖ Original code modules loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è  Original code not available: {e}")
    ORIGINAL_AVAILABLE = False


class OriginalVsNewBenchmark:
    """
    Comprehensive benchmark comparing new package against original code.
    
    Validates physics accuracy, performance, and trajectory consistency
    between research code and production package.
    """
    
    def __init__(self):
        self.new_integrator = Li√©nardWiechertIntegrator()
        self.optimized_integrator = OptimizedLi√©nardWiechertIntegrator()
        
        self.benchmark_results = {
            'single_step_comparison': {},
            'trajectory_comparison': {},
            'energy_conservation_comparison': {},
            'performance_comparison': {},
            'overall_validation': {}
        }
        
    def create_test_particle_system(self, system_type: str = 'two_particle') -> Dict[str, np.ndarray]:
        """
        Create standardized test particle system for benchmarking.
        
        Args:
            system_type: Type of system ('two_particle', 'collision', 'ring')
            
        Returns:
            Particle data dictionary compatible with both implementations
        """
        if system_type == 'two_particle':
            # Simple two-particle system for validation
            particles = {
                'x': np.array([0.0, 1e-6]),      # 1 Œºm separation
                'y': np.array([0.0, 0.0]),
                'z': np.array([0.0, 0.0]),
                't': np.array([0.0, 0.0]),
                'Px': np.array([0.0, 0.0]),      # Start at rest
                'Py': np.array([0.0, 0.0]),
                'Pz': np.array([938.3, 938.3]),  # Rest energy 
                'Pt': np.array([938.3, 938.3]),
                'gamma': np.array([1.0, 1.0]),   # Non-relativistic
                'bx': np.array([0.0, 0.0]),      # Œ≤ = v/c
                'by': np.array([0.0, 0.0]),
                'bz': np.array([0.0, 0.0]),
                'bdotx': np.array([0.0, 0.0]),   # Acceleration
                'bdoty': np.array([0.0, 0.0]),
                'bdotz': np.array([0.0, 0.0]),
                'q': 1.0,           # Elementary charge
                'char_time': np.array([1e-4, 1e-4]),
                'm': 938.3          # Proton mass (MeV)
            }
            
        elif system_type == 'collision':
            # Head-on collision with 0.1c approach velocity
            v_approach = 0.1  # 0.1c
            gamma = 1.0 / np.sqrt(1 - v_approach**2)
            
            particles = {
                'x': np.array([-5e-6, 5e-6]),    # Start 10 Œºm apart
                'y': np.array([0.0, 0.0]),
                'z': np.array([0.0, 0.0]),
                't': np.array([0.0, 0.0]),
                'Px': np.array([gamma * PROTON_MASS * v_approach * C_MMNS,
                               -gamma * PROTON_MASS * v_approach * C_MMNS]),
                'Py': np.array([0.0, 0.0]),
                'Pz': np.array([0.0, 0.0]),
                'Pt': np.array([gamma * PROTON_MASS * C_MMNS**2,
                               gamma * PROTON_MASS * C_MMNS**2]),
                'gamma': np.array([gamma, gamma]),
                'bx': np.array([v_approach, -v_approach]),
                'by': np.array([0.0, 0.0]),
                'bz': np.array([0.0, 0.0]),
                'bdotx': np.array([0.0, 0.0]),
                'bdoty': np.array([0.0, 0.0]),
                'bdotz': np.array([0.0, 0.0]),
                'q': 1.0,
                'char_time': np.array([1e-4, 1e-4]),
                'm': 938.3
            }
            
        return particles
    
    def run_original_integration_step(self, particles: Dict[str, np.ndarray], h: float) -> Dict[str, np.ndarray]:
        """
        Run single integration step using original code.
        
        Args:
            particles: Particle data
            h: Timestep
            
        Returns:
            Updated particle data from original implementation
        """
        if not ORIGINAL_AVAILABLE:
            raise RuntimeError("Original code not available for comparison")
            
        try:
            # Use original static integration function with required parameters
            # apt_R: aperture radius (set to infinity for free space)
            # sim_type: simulation type (0 = conducting flat, 2 = driver)
            apt_R = np.inf  # No aperture restrictions for benchmark
            sim_type = 2    # Driver mode (no wall interactions)
            result = original_lib.eqsofmotion_static(h, particles, particles, apt_R, sim_type)
            return result
            
        except Exception as e:
            print(f"Error running original integration: {e}")
            # Return input if original fails
            return particles
    
    def run_new_integration_step(self, particles: Dict[str, np.ndarray], h: float) -> Dict[str, np.ndarray]:
        """
        Run single integration step using new package.
        
        Args:
            particles: Particle data
            h: Timestep
            
        Returns:
            Updated particle data from new implementation
        """
        return self.new_integrator.eqsofmotion_static(h, particles, particles)
    
    def compare_single_step(self, system_type: str = 'two_particle') -> Dict[str, Any]:
        """
        Compare single integration step between original and new implementations.
        
        Args:
            system_type: Type of particle system to test
            
        Returns:
            Comparison results with relative differences
        """
        print(f"üîç SINGLE STEP COMPARISON - {system_type.upper()}")
        print("="*60)
        
        # Create test system
        particles = self.create_test_particle_system(system_type)
        h = 1e-6  # 1 ns timestep
        
        # Make copies for each integrator
        particles_original = {key: np.copy(val) if isinstance(val, np.ndarray) else val 
                             for key, val in particles.items()}
        particles_new = {key: np.copy(val) if isinstance(val, np.ndarray) else val 
                        for key, val in particles.items()}
        
        print(f"Initial setup:")
        print(f"  System: {system_type}")
        print(f"  Particles: {len(particles['x'])}")
        print(f"  Timestep: {h:.2e} ns")
        print(f"  Separation: {abs(particles['x'][1] - particles['x'][0])*1e6:.1f} nm")
        
        results = {'system_type': system_type, 'timestep': h}
        
        # Run original implementation
        if ORIGINAL_AVAILABLE:
            try:
                start_time = time.time()
                result_original = self.run_original_integration_step(particles_original, h)
                time_original = time.time() - start_time
                
                print(f"\n‚úÖ Original implementation completed in {time_original:.4f}s")
                
                # Calculate momentum changes
                dPx_orig = result_original['Px'][0] - particles['Px'][0]
                dPy_orig = result_original['Py'][0] - particles['Py'][0]
                dPz_orig = result_original['Pz'][0] - particles['Pz'][0]
                dPt_orig = result_original['Pt'][0] - particles['Pt'][0]
                
                print(f"  Original momentum changes:")
                print(f"    ŒîPx = {dPx_orig:.2e} MeV/c")
                print(f"    ŒîPy = {dPy_orig:.2e} MeV/c") 
                print(f"    ŒîPz = {dPz_orig:.2e} MeV/c")
                print(f"    ŒîPt = {dPt_orig:.2e} MeV")
                
                results['original'] = {
                    'time': time_original,
                    'dPx': dPx_orig,
                    'dPy': dPy_orig,
                    'dPz': dPz_orig,
                    'dPt': dPt_orig
                }
                
            except Exception as e:
                print(f"‚ùå Original implementation failed: {e}")
                results['original'] = {'error': str(e)}
                
        else:
            print("‚ö†Ô∏è  Original code not available - skipping comparison")
            results['original'] = {'error': 'Original code not available'}
        
        # Run new implementation
        try:
            start_time = time.time()
            result_new = self.run_new_integration_step(particles_new, h)
            time_new = time.time() - start_time
            
            print(f"\n‚úÖ New implementation completed in {time_new:.4f}s")
            
            # Calculate momentum changes  
            dPx_new = result_new['Px'][0] - particles['Px'][0]
            dPy_new = result_new['Py'][0] - particles['Py'][0]
            dPz_new = result_new['Pz'][0] - particles['Pz'][0]
            dPt_new = result_new['Pt'][0] - particles['Pt'][0]
            
            print(f"  New momentum changes:")
            print(f"    ŒîPx = {dPx_new:.2e} MeV/c")
            print(f"    ŒîPy = {dPy_new:.2e} MeV/c")
            print(f"    ŒîPz = {dPz_new:.2e} MeV/c") 
            print(f"    ŒîPt = {dPt_new:.2e} MeV")
            
            results['new'] = {
                'time': time_new,
                'dPx': dPx_new,
                'dPy': dPy_new,
                'dPz': dPz_new,
                'dPt': dPt_new
            }
            
            # Performance comparison
            if ORIGINAL_AVAILABLE and 'error' not in results['original']:
                speedup = results['original']['time'] / time_new
                print(f"\nüìä Performance comparison:")
                print(f"  Original: {results['original']['time']:.4f}s")
                print(f"  New: {time_new:.4f}s")
                print(f"  Speedup: {speedup:.2f}x")
                results['speedup'] = speedup
                
        except Exception as e:
            print(f"‚ùå New implementation failed: {e}")
            results['new'] = {'error': str(e)}
        
        # Physics comparison
        if (ORIGINAL_AVAILABLE and 'error' not in results.get('original', {}) and 
            'error' not in results.get('new', {})):
            
            print(f"\nüî¨ Physics comparison:")
            
            # Calculate relative differences
            rel_diff_Px = abs(results['new']['dPx'] - results['original']['dPx']) / abs(results['original']['dPx']) if results['original']['dPx'] != 0 else 0
            rel_diff_Py = abs(results['new']['dPy'] - results['original']['dPy']) / abs(results['original']['dPy']) if results['original']['dPy'] != 0 else 0
            rel_diff_Pz = abs(results['new']['dPz'] - results['original']['dPz']) / abs(results['original']['dPz']) if results['original']['dPz'] != 0 else 0
            rel_diff_Pt = abs(results['new']['dPt'] - results['original']['dPt']) / abs(results['original']['dPt']) if results['original']['dPt'] != 0 else 0
            
            print(f"  Relative differences:")
            print(f"    ŒîPx: {rel_diff_Px:.2e} ({rel_diff_Px*100:.4f}%)")
            print(f"    ŒîPy: {rel_diff_Py:.2e} ({rel_diff_Py*100:.4f}%)")
            print(f"    ŒîPz: {rel_diff_Pz:.2e} ({rel_diff_Pz*100:.4f}%)")
            print(f"    ŒîPt: {rel_diff_Pt:.2e} ({rel_diff_Pt*100:.4f}%)")
            
            max_rel_diff = max(rel_diff_Px, rel_diff_Py, rel_diff_Pz, rel_diff_Pt)
            
            results['physics_comparison'] = {
                'rel_diff_Px': rel_diff_Px,
                'rel_diff_Py': rel_diff_Py, 
                'rel_diff_Pz': rel_diff_Pz,
                'rel_diff_Pt': rel_diff_Pt,
                'max_rel_diff': max_rel_diff
            }
            
            # Validation assessment
            if max_rel_diff < 1e-10:
                print("  ‚úÖ EXCELLENT: Machine precision agreement")
                results['validation'] = 'EXCELLENT'
            elif max_rel_diff < 1e-6:
                print("  ‚úÖ VERY GOOD: Numerical precision agreement")
                results['validation'] = 'VERY GOOD'
            elif max_rel_diff < 1e-3:
                print("  ‚úÖ GOOD: Acceptable precision")
                results['validation'] = 'GOOD'
            else:
                print("  ‚ö†Ô∏è  WARNING: Significant differences detected")
                results['validation'] = 'WARNING'
        
        return results
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """
        Run complete benchmark suite comparing original vs new implementations.
        
        Returns:
            Comprehensive benchmark results
        """
        print("üèÅ COMPREHENSIVE ORIGINAL vs NEW BENCHMARK")
        print("="*80)
        print("Validating new package against original research code")
        print()
        
        # Test different system configurations
        test_systems = ['two_particle', 'collision']
        
        for system_type in test_systems:
            try:
                print(f"\n{'-'*20} {system_type.upper()} SYSTEM {'-'*20}")
                result = self.compare_single_step(system_type)
                self.benchmark_results['single_step_comparison'][system_type] = result
                
                if result.get('validation') == 'WARNING':
                    print(f"‚ö†Ô∏è  WARNING: Significant differences in {system_type} system")
                else:
                    print(f"‚úÖ {system_type} system validation passed")
                    
            except Exception as e:
                print(f"‚ùå {system_type} system benchmark failed: {e}")
                self.benchmark_results['single_step_comparison'][system_type] = {'error': str(e)}
        
        # Overall assessment
        print(f"\n{'='*80}")
        print("üìä BENCHMARK SUMMARY")
        print("="*80)
        
        all_passed = True
        for system_type, result in self.benchmark_results['single_step_comparison'].items():
            if 'error' in result:
                print(f"  ‚ùå {system_type}: FAILED ({result['error']})")
                all_passed = False
            elif result.get('validation') in ['EXCELLENT', 'VERY GOOD', 'GOOD']:
                print(f"  ‚úÖ {system_type}: {result['validation']}")
            else:
                print(f"  ‚ö†Ô∏è  {system_type}: {result.get('validation', 'UNKNOWN')}")
                all_passed = False
        
        if ORIGINAL_AVAILABLE:
            if all_passed:
                print(f"\nüéâ ALL BENCHMARKS PASSED!")
                print("New package produces results consistent with original code.")
                overall_status = "PASSED"
            else:
                print(f"\n‚ö†Ô∏è  SOME BENCHMARKS FAILED")
                print("Review individual test results for details.")
                overall_status = "FAILED"
        else:
            print(f"\n‚ö†Ô∏è  ORIGINAL CODE NOT AVAILABLE")
            print("Cannot perform comparison - new package tests passed independently.")
            overall_status = "NO_COMPARISON"
        
        self.benchmark_results['overall_validation'] = {
            'status': overall_status,
            'original_available': ORIGINAL_AVAILABLE,
            'all_tests_passed': all_passed
        }
        
        return self.benchmark_results


def main():
    """Run the comprehensive benchmark suite."""
    print("üî¨ LW INTEGRATOR: ORIGINAL vs NEW BENCHMARK")
    print("="*80)
    print("Validating new package against original research code")
    print()
    
    benchmark = OriginalVsNewBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    
    # Export results
    import json
    def convert_numpy(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy(item) for item in obj]
        return obj
    
    with open('benchmark_results.json', 'w') as f:
        json.dump(convert_numpy(results), f, indent=2)
    
    print(f"\nüìÑ Benchmark results exported to: benchmark_results.json")
    
    return results


if __name__ == "__main__":
    main()
